{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x1_root_pos, x2_root_pos, x1_root_neg, x2_root_neg, n_sample, test_raito=0.3):\n",
    "    # Generate two sets of data point and label\n",
    "    x1_pos = np.random.rand(n_sample) + x1_root_pos\n",
    "    x2_pos = np.random.rand(n_sample) + x2_root_pos\n",
    "    x1_neg = np.random.rand(n_sample) + x1_root_neg\n",
    "    x2_neg = np.random.rand(n_sample) + x2_root_neg\n",
    "    label = np.concatenate([np.ones(n_sample, dtype=float), np.zeros(n_sample, dtype=float)])\n",
    "    \n",
    "    # Combine features and shuffle\n",
    "    x1_data = np.expand_dims(np.concatenate([x1_pos, x1_neg]), axis=1)\n",
    "    x2_data = np.expand_dims(np.concatenate([x2_pos, x2_neg]), axis=1)\n",
    "    x_data = np.concatenate([x1_data, x2_data], axis=1)\n",
    "    \n",
    "    # Split train test set\n",
    "    x_train, x_test, label_train, label_test = train_test_split(x_data, label, test_size=test_raito, random_state=42)\n",
    "    return x_train, x_test, label_train, label_test\n",
    "\n",
    "\n",
    "def visualize_data(x, label, title=''):\n",
    "    # Plot data using matplot lib\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 0:\n",
    "            plt.plot(x[i][0], x[i][1], 'b+')\n",
    "        elif label[i] == 1:\n",
    "            plt.plot(x[i][0], x[i][1], 'ro')\n",
    "    \n",
    "    plt.ylabel('X1 Feature')\n",
    "    plt.xlabel('X2 Feature')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def draw_model(a, b, c):\n",
    "    # Draw line ax + by + c = 0\n",
    "    x = np.arange(-2, 2, 0.2)\n",
    "    y = (-a * x - c) / b\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parameters():\n",
    "    # TODO 1: Initialize parameters of logistic model\n",
    "    a = tf.Variable(initial_value=tf.random_normal(shape=(), dtype=tf.float32))\n",
    "    b = tf.Variable(initial_value=tf.random_normal(shape=(), dtype=tf.float32))\n",
    "    c = tf.Variable(initial_value=tf.random_normal(shape=(), dtype=tf.float32))\n",
    "    return (a , b , c)\n",
    "    pass\n",
    "\n",
    "def define_cost_func(X1, X2, L, a, b, c, n_sample):\n",
    "    # TODO 2: define hypothesis 'h' and cost function cost 'cost'\n",
    "    # h = sigmoid(ax1 +bx2 + c)\n",
    "    h = tf.sigmoid(a * X1 + b * X2 + c)\n",
    "    cost = -1 * tf.reduce_sum(L * tf.log(h) + (1 - L) * tf.log(1 - h)) / n_sample\n",
    "    return cost\n",
    "\n",
    "def define_optimizer(l_rate, cost_func):\n",
    "    # Define optimizer and initializer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(l_rate).minimize(cost_func)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    return optimizer, initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate and visualize training data\n",
    "n_sample = 30\n",
    "x_train, x_test, label_train, label_test = generate_data(3, 5, 4, 4, n_sample)\n",
    "visualize_data(x_train, label_train, \"Data train raw\")\n",
    "\n",
    "# Step 1b: Normalize Xs and re-visualize training data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(x_train)\n",
    "print(x_train[:, 0])\n",
    "print(x_train[:, 0])\n",
    "print(label_train.shape)\n",
    "\n",
    "visualize_data(x_train, label_train, \"Data train scaled\")\n",
    "visualize_data(x_test, label_test, \"Data test scaled\")\n",
    "\n",
    "# Step 2: Initialize Placeholders for input data\n",
    "X1 = tf.placeholder(np.float32)\n",
    "X2 = tf.placeholder(np.float32)\n",
    "L = tf.placeholder(np.float32)\n",
    "batch_size = tf.placeholder(np.float32)\n",
    "\n",
    "### Step 3: Build up your first model: LOGISTIC REGRESSOR\n",
    "a, b, c = define_parameters()\n",
    "cost = define_cost_func(X1, X2, L, a, b, c, batch_size)\n",
    "\n",
    "# Step 4: Create optimizer\n",
    "l_rate = 0.2\n",
    "optimizer, initializer = define_optimizer(l_rate, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-be6ed6ceac46>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-be6ed6ceac46>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for i in range(50):print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])print(train_x[:10, :10])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(initializer)\n",
    "    for i in range(50):\n",
    "        _, train_cost = sess.run([optimizer, cost], feed_dict={X1: x_train[:, 0], X2: x_train[:, 1], L: label_train, batch_size: len(label_train)})\n",
    "        \n",
    "        a_op = sess.run(a)\n",
    "        b_op = sess.run(b)\n",
    "        c_op = sess.run(c)\n",
    "        \n",
    "        draw_model(a_op, b_op, c_op)\n",
    "        visualize_data(x_train, label_train,\"Step {}, loss = {:.4f}\".format(i, train_cost))\n",
    "\n",
    "    test_cost = sess.run(cost, feed_dict={X1: x_test[:, 0], X2: x_test[:, 1], L: label_test, batch_size: len(label_test)})\n",
    "    print('Optimized variable: a_op = ', a_op)\n",
    "    print('Optimized variable: b_op = ', b_op)\n",
    "    print('Optimized variable: c_op = ', c_op)\n",
    "    draw_model(a_op, b_op, c_op)\n",
    "    visualize_data(x_test, label_test, \"Final prediction cost {:.4f}\".format(test_cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
