{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf9RNF8535LK"
   },
   "source": [
    "# PHÂN TÍCH CẢM XÚC VỚI LSTMs\n",
    "\n",
    "- Điểm: \n",
    "- Nhận xét:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqrndwSl4E0v"
   },
   "source": [
    "## Dẫn nhập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SO53ASD0E04T"
   },
   "source": [
    "### Giới thiệu bài toán\n",
    "Trong thực trạng nhiều công ty thương mại điện tử phát triển nở rộ ngày nay, việc thu thập đánh giá từ các bình luận của người dùng là rất cần thiết. Các bình luận này có chứa nhiều thông tin quan trọng, có thể giúp các trang web dễ dàng lọc, đề xuất các mặt hàng, địa điểm phù hợp với từng người dùng và đánh giá chất lượng sản phẩm cũng như dịch vụ của các đối tác.\n",
    "\n",
    "Với hàng chục nghìn cho tới hàng trăm nghìn bình luận mỗi ngày hiện nay thì việc phân loại các bình luận từ người dùng không phải là điều dễ dàng, và đòi hỏi rất nhiều nhân lực. Cụ thể ở đây, ta quan tâm đến cảm xúc gắn liền với các bình luận, để suy ra thái độ của người dùng và từ đó đánh giá chất lượng các sản phẩm.\n",
    "\n",
    "Cùng sự phát triển của AI và các phương pháp ML, hiện nay chúng ta có thể giải quyết bài toán này bằng các mô hình Deep Learning với độ chính xác có thể sánh ngang với khả năng đọc hiểu của con người.\n",
    "\n",
    "Trong bài tập này, ta sẽ tập trung vào phân tích các bình luận tiếng Việt để tìm ra cảm xúc được thể hiện trong bình luận.\n",
    "![](https://drive.google.com/uc?export=view&id=16m7rYwl40I0CBJKWlu44CyAXEMflcF9R)\n",
    "![](https://drive.google.com/uc?export=view&id=1MSEwafiJ7hIoRD4QEzL5umCK8FqlAojN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upWFfxH3_0fT"
   },
   "source": [
    "### Hướng giải quyết bài toán\n",
    "\n",
    "Ta sẽ dùng mạng **LSTM** (Long Short-Term Memory, dịch thô: Bộ-nhớ Ngắn-hạn Dài) để giải quyết bài toán **Sentiment Analysis**  (dịch thô: Phân-tích Cảm-xúc) trên dữ liệu văn bản.\n",
    "\n",
    "Đầu vào nhận được là một văn bản.\n",
    "\n",
    "Đầu ra cần trả về là loại cảm xúc được thể hiện trong văn bản: tích cực, tiêu cực, hay trung lập (positive - negative - neutral). Trong phạm vi của bài tập này, chúng ta chỉ quan tâm đến hai cảm xúc positive và negative.\n",
    "\n",
    "![caption](https://drive.google.com/uc?export=view&id=1cg-W4IXMKP9jfpCcADKXqXwpgEV19I-f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNNGCdvN_41J"
   },
   "source": [
    "### Cách đánh giá mô hình\n",
    "Mô hình luyện được sẽ được đánh giá trên tập dữ liệu gồm 3000 bình luận (được giấu nhãn) với độ đo F1 score. \n",
    "\n",
    "Các bạn nộp dự đoán của mình qua [Kaggle](https://www.kaggle.com/t/86418ca46d79420a95fb38f9e39bd351)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IPjyVy6_6s9"
   },
   "source": [
    "### Cách tính điểm\n",
    "\n",
    "Điểm tổng của bài tập sẽ là:\n",
    "$$\n",
    "Score = solution\\times{70\\%} + test\\_score \\times{30\\%}\n",
    "$$\n",
    "Trong đó:\n",
    "- solution: điểm của phần bài làm (thang điểm 100)\n",
    "- test_score: điểm trên leaderboard của Kaggle, sẽ được so sánh với  baseline model (mô hình mức cơ sở) (thang điểm 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-anIp23M_9jc"
   },
   "source": [
    "## Tìm hướng giải quyết bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7Qvg_rhAIQ0"
   },
   "source": [
    "### Tập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHT-F7W8Afyz"
   },
   "source": [
    "Trong assignment này, chúng ta sử dụng tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn, trong đó có 15,000 mẫu positive và 15,000 mẫu negative (Nguồn: https://streetcodevn.com/blog/dataset).\n",
    "\n",
    "Tập dữ liệu này có thể được tải bằng đoạn code bên dưới:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RNmZscv_j6F"
   },
   "outputs": [],
   "source": [
    "# Tải data\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "gdd.download_file_from_google_drive(file_id='1eF0jg_8P0NnUiiL3HjBaJ8AC2T1sJXiK', dest_path='./assignment4-data.zip', unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWmmvIAqKbGU"
   },
   "source": [
    "Cấu trúc của folder vừa tải xuống:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTGgNgPoJonU"
   },
   "outputs": [],
   "source": [
    "data_dir = 'course5data'\n",
    "!ls {data_dir} #Các lệnh bắt đầu bằng ! trong colab code cell tương đương các lệnh trong terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wk2jM36BOou-"
   },
   "source": [
    "Trong đó:\n",
    "- **train.csv**: file csv chứa 27,000 mẫu dữ liệu đã gán nhãn\n",
    "- **test.csv**: file chứa 3000 mẫu test dùng để đánh giá\n",
    "- **sample_submission.csv**: file mẫu thể hiện hình thức nộp kết quả\n",
    "- **word_list.npy**: chứa danh sách các từ tiếng Việt đã xử lý\n",
    "- **word_vectors.npy**: chứa vectors  biểu diễn các từ trong **word_list.npy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wxy9QmyTRlbt"
   },
   "source": [
    "### Phân tích bài toán dưới góc nhìn Word Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBdOS_ZxTtdr"
   },
   "source": [
    "Nếu như chúng ta giữ nguyên định dạng kí tự của văn bản đầu vào thì rất khó để thực hiện các thao tác toán học, như tính tích vô hướng (dot product) , đưa vào hàm softmax, hay các thuật toán trên Neural Net như backpropagation.\n",
    "\n",
    "Vậy nên, thay vì sử dụng chuỗi kí tự, ta sẽ chuyển  định dạng của văn bản đầu vào thành các vector biểu diễn, để thuận tiện cho việc tính toán.\n",
    "\n",
    "![Word2Vec](https://drive.google.com/uc?export=view&id=12xKP6pFzDzj-0ToZX-27RjOhRcMFrfeb)\n",
    "\n",
    "Trong hình minh hoạ ở trên, dữ liệu đầu vào là câu \"Tôi ăn ... rồi\" với 16 tiếng. Giả sử ta có thể mã hóa mỗi tiếng thành một vector D chiều, ví dụ như:\n",
    "\n",
    "```\n",
    "Tôi = [.2 0 .8]\n",
    "ăn = [0 .6 0]\n",
    "rồi = [.5 .2 .1]\n",
    "với D = 3\n",
    "```\n",
    "\n",
    "Vậy, cả câu \"Tôi ăn ... rồi\" có thể được biểu diễn bằng một ma trận 16\\*D chiều.\n",
    "\n",
    "Để ánh xạ từ một **từ** sang một **vector**, chúng ta sử dụng một bộ vector biểu diễn từ đã được luyện sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6F8jVr24T--X"
   },
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSTJ7AG4RweC"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_eager_execution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f23b0507741f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Enable Eager Execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_eager_execution'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable Eager Execution\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly() \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_ZsxgGzjew2"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2q48NN3U-Ni"
   },
   "source": [
    "## Chuẩn bị dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fmhw-aZ-bfX6"
   },
   "source": [
    "### Tải dữ liệu \n",
    "Sau đây ta tiến hành tải dữ liệu từ file `train.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPDWEmJabuLs"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9spk9D2Kidp"
   },
   "outputs": [],
   "source": [
    "print('Number of train samples in total:', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZ0S2CRhL1lN"
   },
   "outputs": [],
   "source": [
    "print('Number of positives:', np.sum(train_df['class']==1))\n",
    "print('Number of negatives:', np.sum(train_df['class']==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLrJRUc_b1ri"
   },
   "source": [
    "Dữ liệu gồm 3 cột: `id`, `text` và `class`. Trong đó:\n",
    "- `id`: id của mẫu dữ liệu\n",
    "- `text`: dữ liệu văn bản, đã được tiền xử lý để gộp từ và dấu câu\n",
    "- `class`: nhãn của dữ liệu, 1 là positive, 0 là negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QE8B4brTdirt"
   },
   "source": [
    "Để giúp hiểu rõ hơn về dữ liệu, ta có thể hiển thị một số review bất kỳ như sau. Các bạn có thể thử nhiều lần để thấy sự đa dạng của các bình luận trong tập dữ liệu (về cách gõ, hành văn, ngôn từ, *vân vân*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnqsFCzGdlfj"
   },
   "outputs": [],
   "source": [
    "print('Một review tích cực: ')\n",
    "sample_positive = train_df[train_df['class'] == 1].sample(1)\n",
    "print(sample_positive.loc[sample_positive.index[0], 'text'])\n",
    "\n",
    "print('\\nMột review tiêu cực: ')\n",
    "sample_negative = train_df[train_df['class'] == 0].sample(1)\n",
    "print(sample_negative.loc[sample_negative.index[0], 'text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeyAgWPohQDd"
   },
   "source": [
    "Tiếp theo ta load dữ liệu từ file `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyygFLLXhWeU"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7kUiNEvNFe_"
   },
   "outputs": [],
   "source": [
    "print('Number of test samples in total:', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDqkzU_XVCAx"
   },
   "source": [
    "### Tải bộ word embeddings (vector biểu diễn từ, dịch thô là \"từ nhúng (vào không gian vector)\")\n",
    "Đầu tiên, để có thể biểu diễn một từ bằng một vector, ta sẽ sử dụng mô hình đã được luyện sẵn (pretrained model). Ở đây ta sẽ dùng mô hình tiếng Việt trong fasttext, bộ biểu diễn từ cho 157 ngôn ngữ do Facebook AI Research Lab thực hiện (nguồn: https://fasttext.cc).\n",
    "\n",
    "Tuy nhiên, kích thước của mô hình luyện sẵn khá lớn (khoảng 1,2 GB). Mô hình luyện sẵn bao gồm khoảng 2 triệu từ, mỗi từ lại được biểu diễn dưới dạng một vector 300 chiều.\n",
    "\n",
    "\n",
    "\n",
    "> *Câu hỏi phụ: ước tính độ lớn của một ma trận 2D có kích thước 2000000 x 300, mỗi phần tử là một giá trị float. Kiểm tra với kích thước file được cho biết (1,2 GB).*\n",
    "\n",
    "> **_Các câu hỏi phụ (ở đây và ở dưới) chỉ mang tính gợi mở để các bạn hiểu thêm về bài toán, không yêu cầu trả lời, và cũng không tính vào điểm cuối cùng._**\n",
    "\n",
    "Kích thước lớn của mô hình gây khó khăn cho việc tải và sử dụng nó cho bài toán của chúng ta. Vì vậy, đội giảng dạy đã thu gọn mô hình trên, với số lượng từ vừa đủ để có thể xử lý tập dữ liệu review đồ ăn của Foody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5e3hSngMUGBZ"
   },
   "outputs": [],
   "source": [
    "words_list = np.load(os.path.join(data_dir, 'words_list.npy'))\n",
    "print('Prunned vocabulary loaded!')\n",
    "words_list = words_list.tolist()\n",
    "word_vectors = np.load(os.path.join(data_dir, 'word_vectors.npy'))\n",
    "word_vectors = np.float32(word_vectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d86tOJcEVe0-"
   },
   "source": [
    "Để đảm bảo dữ liệu được tải một cách chính xác, ta nên kiểm tra xem số từ vựng và số chiều của ma trận biểu diễn từ (word embedding) có khớp với nhau hay không? Trong trường hợp này số từ được giữ lại là 19899 và số chiều của mỗi vector biểu diễn là 300 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLYBgqXyVdcx"
   },
   "outputs": [],
   "source": [
    "print('Size of the vocabulary: ', len(words_list))\n",
    "print('Size of the word embedding matrix: ', word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udbdJlblVp2l"
   },
   "source": [
    "### Cách lấy word vector của một từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNPecz9sbpgP"
   },
   "source": [
    "`words_list` và `word_vectors` đã được thiết lập sẵn sao cho vector đầu tiên trong `word_vectors` sẽ tương ứng với từ đầu tiên `words_list`. Tương tự với cặp vector và từ thứ hai, thứ ba, vân vân, đến cuối.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydbng0bvbpgS"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=5)\n",
    "for i in range(5):\n",
    "  print('Index', i, 'thuộc về từ', words_list[i])\n",
    "  print('có biểu diễn vector là', word_vectors[i], '\\tkích thước', word_vectors[i].shape)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBPGJi_obpgh"
   },
   "source": [
    "Theo như trên, để truy xuất vector một từ nhất định thì ta cần biết chỉ số của từ đó. Lấy ví dụ trường hợp ta muốn biết biểu diễn vector của từ \"ngon\" thì ta cần thực hiện các bước sau:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xh-GnieCbpgk"
   },
   "outputs": [],
   "source": [
    "# Bước 1\n",
    "word = 'ngon'\n",
    "  \n",
    "# Bước 2\n",
    "word_idx = words_list.index(word)\n",
    "\n",
    "# Bước 3\n",
    "word_vec = word_vectors[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJiS9s9ybpgt"
   },
   "source": [
    "Tuy nhiên, phương pháp trên khá chậm, và sẽ trở nên rất chậm khi ta cần truy xuất hàng trăm, hàng nghìn vector của các từ trong câu. Khúc mắc nằm ở bước 2: việc tìm index của một từ trong một list (không có phần tử trùng) có độ phức tạp O(n). Ta có thể làm nhanh hơn thế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYiVlb9mbpgx"
   },
   "outputs": [],
   "source": [
    "word2idx = {w:i for i,w in enumerate(words_list)}\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K1PQH1Obpg4"
   },
   "outputs": [],
   "source": [
    "# Bước 1\n",
    "word = 'ngon'\n",
    "\n",
    "# Bước 2\n",
    "word_idx = word2idx[word]\n",
    "\n",
    "# Bước 3\n",
    "word_vec = word_vectors[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSnuCNtebphD"
   },
   "source": [
    "Thông qua dictionary `word2idx`, việc truy xuất chỉ số của một từ bất kì đã giảm độ phức tạp xuống còn O(1). Việc này sẽ giúp tăng tốc quá trình biến đổi tập dữ liệu của chúng ta ở dưới.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BiSxTMTb0uA"
   },
   "source": [
    "### Tiền xử lý văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jo2OtPMzb0t7"
   },
   "source": [
    "Để tiết kiệm thời gian, tập dữ liệu được cung cấp đã được **tách từ** sẵn. Những từ ghép gồm nhiều tiếng sẽ có dấu '_' nối giữa các tiếng, ví dụ: 'sinh_viên', 'máy_chiếu_hình'.\n",
    "\n",
    "> *Tách từ tiếng Việt* là một bài toán không đơn giản, và cũng **nằm ngoài phạm vi của khóa học này**. Trong tiếng Anh, hầu hết các từ đều được tách bằng dấu cách. Tuy nhiên, trong tiếng Việt, dấu cách được dùng để tách biệt các **tiếng** chứ không phải các **từ**, nên đây là một vấn đề quan trọng cần lưu ý khi xử lý văn bản tiếng Việt.\n",
    "\n",
    "\n",
    "\n",
    "Ngoài ra, bạn cũng được cung cấp sẵn các hàm nhằm loại bỏ các ký tự đặc biệt trong văn bản. Tham khảo hàm `clean_sentences` ở dưới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMkBSmahb0td"
   },
   "outputs": [],
   "source": [
    "# Loại bỏ các dấu câu, dấu ngoặc, chấm than chấm hỏi, vân vân..., chỉ chừa lại các kí tự chữ và số\n",
    "import re\n",
    "# re = regular expressions\n",
    "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "def clean_sentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rX_Tybhbzlz"
   },
   "source": [
    "### TODO 1: Viết hàm biểu diễn văn bản thành ma trận biểu diễn từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEjFPtdbX-DG"
   },
   "source": [
    "Tiến thêm một bước từ việc biểu diễn một từ bằng một vector, ở phần này ta sẽ biểu diễn **một câu có nhiều từ** dưới dạng **một ma trận có nhiều vector**.\n",
    "\n",
    "Ví dụ: _\"Món này ăn hoài không biết chán\"_\n",
    "\n",
    "Đầu tiên, với mỗi từ trong câu trên, ta tìm chỉ số (index) tương ứng của từ trong `words_list`, rồi lưu tất cả các chỉ số đó vào một vector đặt tên là `sentence_indices`.\n",
    "\n",
    "Sau đó, chúng ta có thể sử dụng hàm tra cứu ma trận word embedding của thư viện Tensorflow `tf.nn.embedding_lookup` để tra các vector tại các chỉ số trong `sentence_indices`. \n",
    "\n",
    "Nếu ta đặt giới hạn độ dài tối đa của mỗi câu là 10 từ, thì **ma trận biểu diễn câu** sẽ có kích thước *10 x 300*, tương ứng với *10 từ*, mỗi từ là 1 vector *300 phần tử*. Dĩ nhiên, nếu trong tập dữ liệu có nhưng câu dài hơn 10 từ thì ta có thể nâng giới hạn này lên cho phù hợp.\n",
    "\n",
    "![Embedding](https://drive.google.com/uc?export=view&id=1UvvIa22H3RpWCs_kkzYPprjAPY2-9Ov5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sa5Rt8lTeFWK"
   },
   "source": [
    "**Lưu ý**: Trường hợp gặp từ nào không có trong `words_list`, là bộ \"từ điển\" chứa những từ mà ta \"biết\" (tức là có biểu diễn vector), thì ta sẽ gán cho từ đó chỉ số của từ `UNK` (unknown).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzPjHdRtWWHf"
   },
   "outputs": [],
   "source": [
    "def get_sentence_indices(sentence, max_seq_length, _words_list):\n",
    "    \"\"\"\n",
    "    Hàm này dùng để lấy index cho từng từ\n",
    "    trong câu (không có dấu câu, có thể in hoa)\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence là câu cần xử lý\n",
    "    max_seq_length là giới hạn số từ tối đa trong câu\n",
    "    _words_list là bản sao local của words_list, được truyền vào hàm\n",
    "    \"\"\"\n",
    "    indices = np.zeros((max_seq_length), dtype='int32')\n",
    "    \n",
    "    # Tách câu thành từng tiếng\n",
    "    words = [word.lower() for word in sentence.split()]\n",
    "    \n",
    "    # Lấy chỉ số của UNK\n",
    "    unk_idx = word2idx['UNK']\n",
    "    \n",
    "    ### TODO 1 ###\n",
    "    # Viết lệnh cần thực hiện trong vòng lặp dưới [tgay vào pass]\n",
    "    # Vòng lặp chạy qua tất cả các phần tử trong list `words`, tức các từ trong câu\n",
    "    # vd: iter 1: idx = 1, word = tôi\n",
    "    #     iter 2: idx = 2, word = đi\n",
    "    #     iter 3: idx = 3, word = học\n",
    "    #     iter 4: v.v\n",
    "    # Trong mỗi vòng lặp, hãy thay các phần tử tương ứng\n",
    "    # của `indices` bằng các chỉ số/index của các từ trong câu\n",
    "    # LƯU Ý 1: len(indices) có thể ngắn hơn len(words)\n",
    "    # LƯU Ý 2: câu có thể chứa những từ out of vocabulary (OOV), tức không có trong _word_list\n",
    "    ### START CODE HERE ###\n",
    "    for idx, word in enumerate(words):\n",
    "        pass\n",
    "    ### END CODE HERE ###\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-xtEqv_ZXaI"
   },
   "source": [
    "Ở ví dụ dưới đây, ta kiểm tra lại hàm vừa cài đặt ở TODO 1. Nếu bạn thực hiện đúng thì vector `sentence_indices` sẽ có giá trị là: `[119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4gNv4udZSn-"
   },
   "outputs": [],
   "source": [
    "# Ví dụ:\n",
    "sentence = \"Món này ăn hoài không biết chán\"\n",
    "\n",
    "# Tiền xử lý câu\n",
    "sentence = clean_sentences(sentence)\n",
    "\n",
    "# Lấy index của từng từ\n",
    "sentence_indices = get_sentence_indices(sentence, max_seq_length=10, _words_list=words_list)\n",
    "\n",
    "# Hiển thị các chỉ số tương ứng với các từ trong câu\n",
    "for i in range(len(sentence.split())):\n",
    "  print(sentence.split()[i],'--->',sentence_indices[i])\n",
    "\n",
    "# Hiển thị đầy đủ sentence_indices\n",
    "print(sentence_indices)\n",
    "print('Chú ý: các chỉ số thứ 7, 8, 9 của sentence_indices bằng 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKdaY_Z2SBrL"
   },
   "outputs": [],
   "source": [
    "# Ma trận biểu diễn:\n",
    "print('Vector representation of sentence:')\n",
    "print(tf.nn.embedding_lookup(word_vectors,sentence_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwvNkbiqbB3T"
   },
   "source": [
    "### Khảo sát tập dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gqrJmKhbHNy"
   },
   "source": [
    "Với khối lượng dữ liệu lớn (27,000 mẫu), nếu chúng ta chọn đặt **giới hạn số từ tối đa cho một câu** (`MAX_SEQ_LENGTH`) quá cao thì sẽ phí bộ nhớ khi biểu diễn những câu review quá ngắn. Ngược lại, nếu đặt giới hạn quá thấp thì ở những câu dài, ta sẽ bị mất các từ cuối, có khả năng ảnh hưởng đến việc suy đoán cảm xúc.\n",
    "\n",
    "Vì vậy, ta cần thống kê độ dài của các mẫu dữ liệu huấn luyện.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLZQxkJcaVu3"
   },
   "outputs": [],
   "source": [
    "num_words = [len(clean_sentences(x).split()) for x in list(train_df['text'])]\n",
    "print('The total number of samples is', len(train_df))\n",
    "print('The total number of words in the files is', sum(num_words))\n",
    "print('The average number of words in the files is', sum(num_words)/len(num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkMPLGXLc4_8"
   },
   "source": [
    "Chúng ta có thể sử dụng thư viện Matplotlib để minh họa phân bố về chiều dài của các câu review trong tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C62SMwSvcvAg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(num_words, 100)\n",
    "plt.xlabel('Số từ trong câu')\n",
    "plt.ylabel('Tần số')\n",
    "plt.axis([0, 600, 0, 5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GeIifGzCdDBs"
   },
   "source": [
    "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy chọn đặt giới hạn `MAX_SEQ_LENGTH` 200 là tương đối hợp lý. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MLa3ZhQdBNY"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGJSIDQydwjD"
   },
   "source": [
    "### Chuyển dữ liệu văn bản thành ma trận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErpT7GSLeQ7Z"
   },
   "source": [
    "Trong phần này, chúng ta sẽ tiến hành biến đổi text của review thành vector index của các từ trong `words_list`. Tập hợp các vector của `train_df` sẽ tạo thành ma trận `train_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLHgPdOHdVBM"
   },
   "outputs": [],
   "source": [
    "def text2ids(df, max_length, _word_list):\n",
    "    \"\"\"\n",
    "    Biến đổi các text trong dataframe thành ma trận index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        dataframe chứa các text cần biến đổi\n",
    "    max_length: int\n",
    "        độ dài tối đa của một text\n",
    "    _word_list: numpy.array\n",
    "        array chứa các từ trong word vectors\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.array\n",
    "        len(df) x max_length contains indices of text\n",
    "    \"\"\"\n",
    "    ids = np.zeros((len(df), max_length), dtype='int32')\n",
    "    for idx, text in enumerate(tqdm(df['text'])):\n",
    "        ids[idx,:] = get_sentence_indices(clean_sentences(text), max_length, _word_list)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQpKnc3jMqNa"
   },
   "outputs": [],
   "source": [
    "# LƯU Ý: Bước thực hiện này tương đối mất thời gian.\n",
    "# Nếu đã có file train_ids.npy rồi thì ra có thể load lên\n",
    "# và sử dụng luôn ở bước tiếp theo\n",
    "\n",
    "print(\"Converting train_df...\")\n",
    "train_ids = text2ids(train_df, MAX_SEQ_LENGTH, words_list)\n",
    "np.save('train_ids.npy', train_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fdCtmneNqZw"
   },
   "outputs": [],
   "source": [
    "# Trường hợp đã tính toán và lưu ma trận rồi thì ta có thể load lên\n",
    "train_ids = np.load('train_ids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0yreBE3OCLf"
   },
   "source": [
    "In thử word indices của review đầu tiên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jk-esTk0OO5p"
   },
   "outputs": [],
   "source": [
    "print('Word indices of the first review: ')\n",
    "print(train_ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1h6E82p0PiLn"
   },
   "source": [
    "### Chia dữ liệu thành train, validation và test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2-eOZfKPrjo"
   },
   "source": [
    "Chia dữ liệu `train_ids` và cột `class` của `train_df` thành 3 phần **train : validation : test = 0.8 : 0.1 : 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoBzDNS6PqBH"
   },
   "outputs": [],
   "source": [
    "train_x, test_validation_x, train_y, test_validation_y  = train_test_split(train_ids, train_df['class'], test_size=0.2, random_state=2019)\n",
    "validation_x, test_x, validation_y, test_y = train_test_split(test_validation_x, test_validation_y, test_size=0.5, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dcdjv-OhQD9N"
   },
   "source": [
    "### TODO 2: Khởi tạo các `train_dataset`, `validation_dataset`, `test_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhSxr4r7Qi92"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 # Có thể chạy trên Tesla K80 12GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQTvO7lcQQ3C"
   },
   "outputs": [],
   "source": [
    "### TODO 2\n",
    "# Sử dụng các biến đã tạo ở trên, hình thành 3 tập dataset\n",
    "# train_dataset, validation_dataset, test_dataset\n",
    "#\n",
    "# GỢI Ý:\n",
    "# Cả 3 tập đều có quy trình tạo như nhau, chỉ khác về tham biến.\n",
    "#\n",
    "# Một dataset được tạo như sau:\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((features,targets))\n",
    "#\n",
    "# Các dataset sau đó cần được chia thành batch.\n",
    "# dataset = dataset.batch(batch_size)\n",
    "#\n",
    "# Tham khảo: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "### START CODE HERE\n",
    "train_dataset = None # train_x, train_y\n",
    "validation_dataset = None # validation_x, validation_y\n",
    "test_dataset = None # test_x, test_y\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeYWgkmVSfW1"
   },
   "source": [
    "Chạy dòng lệnh sau để kiểm tra TODO 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1mQPWZnRy08"
   },
   "outputs": [],
   "source": [
    "for idx, (x,y) in enumerate(train_dataset):\n",
    "    if idx == 0:\n",
    "        print('FIRST BATCH:')\n",
    "        print('X =',x)\n",
    "        print('y =',y)\n",
    "print(\"Total: \", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rdROuRjTK3s"
   },
   "source": [
    "Nếu cài đặt đúng, kết quả sẽ có dạng như sau:\n",
    "```\n",
    "FIRST BATCH:\n",
    "X = tf.Tensor(\n",
    "[[12844  5596  4884 ...     0     0     0]\n",
    " [10774 16521 13952 ...     0     0     0]\n",
    " [ 7446 10698 10774 ...     0     0     0]\n",
    " ...\n",
    " [ 4884 16995  4601 ...     0     0     0]\n",
    " [ 3913 15085 14017 ...     0     0     0]\n",
    " [ 4788 14598   310 ...     0     0     0]], shape=(256, 200), dtype=int32)\n",
    "y = tf.Tensor(\n",
    "[0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0\n",
    " 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
    " 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1\n",
    " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
    " 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0\n",
    " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0\n",
    " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1], shape=(256,), dtype=int32)\n",
    "Total:  84\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cg5jiv6ldW6h"
   },
   "source": [
    "> *Câu hỏi phụ: số 84 trong kết quả trên nghĩa là gì? Liên hệ trở lại với BATCH_SIZE.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrtvxzmBlU5b"
   },
   "source": [
    "### Tổng kết quá trình chuẩn bị dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iahwhDpVUu1o"
   },
   "source": [
    "Quá trình kết hợp ma trận chỉ số (indices) và ma trận biểu diễn từ (word embeddings) sang data tensor dùng để huấn luyện được mô tả như hình dưới đây:\n",
    "![caption](https://drive.google.com/uc?export=view&id=1SST9hIt4jboWYr4A41-wI2jMbKI25A9e)\n",
    "\n",
    "Từ ma trận indices và ma trận embedding, sử dụng `tf.nn.embedding_lookup` ta thu được ma trận có kích thước `(BATCH_SIZE, MAX_SEQUENCE_LENGTH, WORD_VEC_DIM)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tZmse4XJDo_"
   },
   "outputs": [],
   "source": [
    "for idx, (x,y) in enumerate(train_dataset):\n",
    "    if idx == 0:\n",
    "        print('EMBEDDING OF FIRST BATCH:')\n",
    "        print(tf.nn.embedding_lookup(word_vectors, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yJHrV1XTSvm"
   },
   "source": [
    "Chú ý vào phần shape của tensor output ở trên, ta có thể thấy kích thước của từng batch dữ liệu, tương ứng với hình minh họa trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rENHGOAuJDGo"
   },
   "source": [
    "Vậy là ta đã hoàn thành quá trình chuẩn bị dữ liệu!\n",
    "\n",
    "Tiếp theo, ta sẽ xây dựng mô hình RNN để huấn luyện trên dữ liệu này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbGEuVimsf-L"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQP1QMcKT0Kf"
   },
   "source": [
    "## Xây dựng mô hình RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BRkJI6HlOQ3"
   },
   "source": [
    "Trong bài tập này, ta sẽ sử dụng mạng Long Short-Term Memory (LSTM) - Bộ nhớ Ngắn hạn Dài, một dạng kiến trúc RNN để gỉải quyết bài toán phân tích cảm xúc.\n",
    "\n",
    "**Kiến trúc mạng LSTM** ta sẽ sử dụng trong bài tập này được mô tả như hình dưới. Để tăng độ phức tạp của mô hình, ta chồng nhiều tầng LSTM lên nhau (**Stacked LSTM layers**). Giữa tầng LSTM dưới và tầng LSTM trên, ta đặt một lớp dropout.\n",
    "\n",
    "Việc chồng thêm các tầng LSTM sẽ giúp cho mô hình có khả năng \"suy luận\" ở những mức độ trừu tượng/abstraction cao hơn (tương tự như khi dùng nhiều tầng CNN), nhưng đồng thời cũng làm tăng số lượng tham số, đồng nghĩa với việc làm tăng đô phức tạp của mô hình và thời gian huấn luyện.\n",
    "\n",
    "![caption](https://drive.google.com/uc?export=view&id=10m5sPPmba__bHx6V9Wp3fp1Wot5uOhBG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2h7lPbXY-1R"
   },
   "source": [
    "### Chọn  LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtKZQCvQZRYc"
   },
   "source": [
    "Để hỗ trợ tốt nhất cho việc tính toán trên GPU, keras hỗ trợ hai loại LSTM layer: 1 loại dành được tối ưu cho các tính toán trên GPU, và một loại dành cho CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUZZ4d5nTIp4"
   },
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print('Using GPU LSTM')\n",
    "    lstm_layer = tf.keras.layers.CuDNNLSTM\n",
    "else:\n",
    "    print('Using CPU LSTM')\n",
    "    import functools\n",
    "    lstm_layer = functools.partial(\n",
    "            tf.keras.layers.LSTM, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSbwoc2OZr9v"
   },
   "source": [
    "### TODO 3: Định nghĩa và tạo mô hình\n",
    "Ở bước này, chúng ta cần định nghĩa mô hình `SentimentAnalysisModel`, kế thừa từ `tf.keras.Model`. Chúng ta sẽ áp dụng mô hình stacked LSTM ở trên để xây dựng `SentimentAnalysisModel`.\n",
    "\n",
    "Các bạn theo dõi hướng dẫn cụ thể trong code ở dưới.\n",
    "\n",
    "Hoàn thành hàm `__init__` để tạo định nghĩa của `class SentimentAnalysisModel`.\n",
    "\n",
    "Hoàn thành hàm `call` để tạo lệnh feedforward khi gọi một `object` của `class SentimentAnalysisModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZNbIr7GZncE"
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Mô hình phân tích cảm xúc của câu\n",
    "    \n",
    "    Properties\n",
    "    ----------\n",
    "    word2vec: numpy.array\n",
    "        word vectors \n",
    "    lstm_layers: list\n",
    "        list of lstm layers, lstm cuối cùng sẽ chỉ trả về output của lstm cuối cùng\n",
    "    dropout_layers: list\n",
    "        list of dropout layers\n",
    "    dense_layer: Keras Dense Layer\n",
    "        lớp dense layer cuối cùng nhận input từ lstm, \n",
    "        đưa ra output bằng số lượng class thông qua hàm softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, word2vec, lstm_units, n_layers, num_classes, dropout_rate=0.25):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        word2vec: numpy.array\n",
    "            word vectors \n",
    "        lstm_units: int\n",
    "            số đơn vị lstm\n",
    "        n_layers: int\n",
    "            số layer lstm xếp chồng lên nhau\n",
    "        num_classes: int\n",
    "            số class đầu ra\n",
    "        dropout_rate: float\n",
    "            tỉ lệ dropout giữa các lớp\n",
    "        \"\"\"\n",
    "        super().__init__(name='sentiment_analysis')\n",
    "        \n",
    "        # Khởi tạo các đặc tính của model\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "        self.lstm_layers = []  # List chứa các tầng LSTM\n",
    "        self.dropout_layers = []  # List chứa các tầng dropout\n",
    "\n",
    "        ### TODO 3.1\n",
    "        # Vòng lặp dưới chạy qua N tầng trong stack\n",
    "        # mỗi tầng sẽ có 1 lstm_layer và 1 dropout_layer\n",
    "        # \n",
    "        # Khởi tạo lstm_layer (GPU) với các tham biến sau:\n",
    "        # lstm_layer(units=..., return_sequences=[True/False])\n",
    "        # Tham khảo: https://keras.io/layers/recurrent/#cudnnlstm\n",
    "        #\n",
    "        # Khởi tạo lstm_layer (CPU) với các tham biến sau:\n",
    "        # lstm_layer(units=..., return_sequences=[True/False])\n",
    "        # Tham khảo: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "        # \n",
    "        # LƯU Ý:\n",
    "        # return_sequence của lstm_layer nhận giá trị True ở mọi tầng  \n",
    "        # trong stack, ngoại trừ tầng cuối cùngg\n",
    "        #\n",
    "        # Khởi tạo dropout_layer với các tham biến sau:\n",
    "        # new_dropout = tf.keras.layers.Dropout(rate=...)\n",
    "        # Tham khảo: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "        # \n",
    "        # Sau khi khởi tạo lstm_layer và dropout_layer\n",
    "        # hãy thêm chúng vào 2 list tương ứng\n",
    "        # self.lstm_layers và self.dropout_layers\n",
    "        #\n",
    "        # Cuối cùng, khởi tạo tầng fully-connected/dense\n",
    "        # tf.keras.layers.Dense(num_classes=..., activation=' ')\n",
    "        # Tham khảo: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "        #\n",
    "        ### START CODE HERE\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            new_lstm = None\n",
    "            self.lstm_layers.append(new_lstm)\n",
    "            new_dropout = None\n",
    "            self.dropout_layers.append(new_dropout)\n",
    "        \n",
    "        self.dense_layer = None\n",
    "        ### END CODE HERE\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        ### TODO 3.2\n",
    "        # Thực hiện các bước biến đổi khi truyền thuận input qua mạng\n",
    "        # Điền code vào các đoạn pass\n",
    "        ### START CODE HERE\n",
    "        inputs = tf.cast(inputs, tf.int32)\n",
    "        # Input hiện là indices, cần chuyển sang dạng vector\n",
    "        # sử dụng:\n",
    "        # tf.nn.embeddings_lookup(embeddings, indices)\n",
    "        pass       \n",
    "      \n",
    "        # Truyền thuận inputs lần lượt qua các tầng\n",
    "        # ở mỗi tầng, truyền input qua các layer: lstm > dropout\n",
    "        # vd: x = first_lstm(x)\n",
    "        #     x = first_dropout(x)\n",
    "        #     x = second_lstm(x)\n",
    "        #     v.v.\n",
    "        pass\n",
    "     \n",
    "        # Gán giá trị tầng cuối cùng vào out và trả về\n",
    "        out = None\n",
    "        \n",
    "        return out\n",
    "        ### END CODE HERE\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVrxaBOujZTB"
   },
   "source": [
    "Tiếp theo, ta lựa chọn các hyperparameters (siêu tham số) phù hợp, rồi tạo mô hình dựa trên định nghĩa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2a3sSbPxfuD0"
   },
   "outputs": [],
   "source": [
    "# Các hyperparameters\n",
    "LSTM_UNITS = 128\n",
    "N_LAYERS = 2\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNJgl7bAfqaq"
   },
   "outputs": [],
   "source": [
    "model = SentimentAnalysisModel(word_vectors, LSTM_UNITS, N_LAYERS, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7LuqOgplKAr"
   },
   "source": [
    "> *Câu hỏi phụ: Hãy thử ước tính xem mô hình với các siêu tham số trên sẽ có bao nhiêu tham số cần huấn luyện?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IUyoz7Mmi36"
   },
   "source": [
    "### Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gThlJieBhjcr"
   },
   "source": [
    "Ở bài tập 3 (CNN), ta đã sử dụng hàm `fit` của keras model trong quá trình huấn luyện.\n",
    "\n",
    "Ở bài tập này, quá trình huấn luyện được thực hiện ở cấp độ thấp hơn (lower level), và ta sẽ đi qua từng bước: **truyền thuận**, **truyền ngược**, và **tối ưu hoá**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTCY-93vL1ur"
   },
   "source": [
    "Đầu tiên, chọn các tham số huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVHRXfewLwFv"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wM_zGomKL7dc"
   },
   "source": [
    "Ta sẽ sử dụng thuật toán tối ưu hoá Adam với learning rate là `LEARNING_RATE`:\n",
    "\n",
    "> Tham khảo thêm về Adam ở các link dưới:\n",
    ">  * tiếng Anh: https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam\n",
    ">  * tiếng Anh: http://ruder.io/optimizing-gradient-descent/index.html#adam\n",
    ">  * tiếng Việt: https://viblo.asia/p/thuat-toan-toi-uu-adam-aWj53k8Q56m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jx1pYLwDv5sD"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10hSNxcUeSqt"
   },
   "source": [
    "Các biến khác hỗ trợ cho việc huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPTbllQyHaVt"
   },
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "# Thư mục chứa checkpoint\n",
    "checkpoint_dir = './model'\n",
    "# Mẫu tên checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2iyx0MKMo_0"
   },
   "source": [
    "#### TODO 4: Viết hàm huấn luyện từng epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCzyQPxLNlaw"
   },
   "source": [
    "Trước tiên, ta sẽ import vài thành phần từ thư viện `fastprogress` để tiện cho việc biểu diễn \n",
    "quy trình huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApouuAjoPIuy"
   },
   "outputs": [],
   "source": [
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CKrlwKvM0d4"
   },
   "source": [
    "Ta cần xây dựng hàm `epoch_training`, thực hiện các bước **truyền thuận**, **truyền ngược** và **tối ưu** tại mỗi epoch.\n",
    "\n",
    "Hàm `epoch_training`:\n",
    "* nhận các tham biến `model`, `dataset`, `global_step`, `mb`, và `num_step`\n",
    "* trả về giá trị loss trung bình của model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uat4rr_ONBpL"
   },
   "outputs": [],
   "source": [
    "def epoch_training(model, dataset, global_step, mb, num_step):\n",
    "    \"\"\"\n",
    "    Huấn luyện mô hình qua 1 epoch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tf.keras.Model\n",
    "        model cần huấn luyện\n",
    "    dataset: tf.Dataset\n",
    "        dataset dùng để huấn luyện\n",
    "    global_step: global step \n",
    "    mb: fastprogress.master_bar\n",
    "        thanh biểu diễn tiến độ\n",
    "    num_step: int\n",
    "        số bước lặp trong mỗi epoch\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Giá trị loss trung bình của epoch\n",
    "    \"\"\"\n",
    "    train_losses = [] # Chứa giá trị loss của các batch\n",
    "    dataset_iter = iter(dataset)\n",
    "    for _ in progress_bar(range(num_step), parent=mb):\n",
    "        inp, target = next(dataset_iter)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            ### TODO 4.1\n",
    "            # Truyền thuận - Feedforward\n",
    "            # Để tạo giá trị predictions\n",
    "            ### START CODE HERE\n",
    "            predictions = None\n",
    "            ### END CODE HERE\n",
    "            \n",
    "            ### TODO 4.2\n",
    "            # Sử dụng tf.losses.sparse_softmax_cross_entropy để tính loss\n",
    "            # Tham khảo: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/losses/sparse_softmax_cross_entropy\n",
    "            ### START CODE HERE\n",
    "            loss = 0\n",
    "            ### END CODE HERE\n",
    "          \n",
    "            # Thêm loss của batch vào train_losses\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "        \n",
    "        \n",
    "        ### TODO 4.3\n",
    "        #  Truyền ngược - Backprop: tính gradient của loss\n",
    "        # theo các tham số mô hình (model.trainable_variables)\n",
    "        # sử dụng tape.gradient\n",
    "        # Tham khảo: https://www.tensorflow.org/api_docs/python/tf/GradientTape#gradient\n",
    "        ### START CODE HERE\n",
    "        grads = None\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        ### TODO 4.4\n",
    "        # Dùng gradient để tối ưu các tham số\n",
    "        # sử dụng optimizer.apply_gradients\n",
    "        # vd: optimizer.apply_gradients(zip(dloss_over_dvars, vars), global_step = global_step)\n",
    "        # LƯU Ý: Không cần gán giá trị của optimizer.apply_gradients cho biến\n",
    "        # Tham khảo: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#apply_gradients\n",
    "        ### START CODE HERE\n",
    "               \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        mb.child.comment = 'Train loss {:.4f}'.format(loss)\n",
    "        \n",
    "    # Trả về loss trung bình\n",
    "    return sum(train_losses)/ len(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7l17sNSRgSg"
   },
   "source": [
    "#### TODO 5: Viết hàm đánh giá từng epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pp1PF5BdRoJO"
   },
   "source": [
    "Ta cần xây dựng hàm `epoch_evaluation`, thực hiện duy nhất bước **truyền thuận** để tính loss.\n",
    "\n",
    "Hàm `epoch_evaluation`:\n",
    "* nhận vào `model`, `dataset`, `mb`, và `num_step`\n",
    "* trả về giá trị `loss`, `f1_score` của model trên tập `dataset` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XiNIW2mWYzo"
   },
   "outputs": [],
   "source": [
    "def epoch_evaluation(model, dataset, mb, num_step):\n",
    "    \"\"\"\n",
    "    Đánh giá mô hình qua 1 epoch\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tf.keras.Model\n",
    "        model cần đánh giá\n",
    "    dataset: tf.Dataset\n",
    "        dataset cần đánh giá\n",
    "    mb: fastprogress.master_bar\n",
    "        progess bar\n",
    "    num_step: int\n",
    "        số bước lặp trong mỗi epoch\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Giá trị loss trung bình của epoch\n",
    "    float\n",
    "        Giá trị F1-score trên tập validation\n",
    "    \"\"\"\n",
    "    val_losses = []   # Chứa giá trị loss của các batch\n",
    "    val_gt = []       # Chứa giá trị nhãn thật của tập validation , gt = groundtruth\n",
    "    val_pred = []     # Chứa giá trị nhãn dự đoán do model trên tập validation\n",
    "    dataset_iter = iter(dataset)\n",
    "    for _ in progress_bar(range(num_step), parent=mb):\n",
    "        inp, target = next(dataset_iter)\n",
    "        \n",
    "        val_gt.extend(target.numpy().astype(np.int32).tolist())\n",
    "        ### TODO 5.1\n",
    "        # Truyền thuận\n",
    "        # để lấy predictions\n",
    "        ### START CODE HERE\n",
    "        predictions = None\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        ### TODO 5.2\n",
    "        # Tính loss giữa target và predictions\n",
    "        # sử dụng hàm loss sparse_softmax_cross_entropy\n",
    "        # như TODO 4.2\n",
    "        ### START CODE HERE\n",
    "        loss = 0\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        \n",
    "        # Thêm loss vào val_losses\n",
    "        val_losses.append(loss)\n",
    "        \n",
    "        val_pred.extend(tf.argmax(predictions,1).numpy().astype(np.int32).tolist())\n",
    "        \n",
    "        mb.child.comment = 'Validation loss {:.4f}'.format(loss)\n",
    "    \n",
    "    # Trả về loss trung bình và F1-score\n",
    "    return sum(val_losses)/ len(val_losses), f1_score(val_gt, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yZ5AQqlZNL4"
   },
   "source": [
    "#### TODO 6: Thiết lập quy trình huấn luyện\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFl2Cona57Yq"
   },
   "source": [
    "Khi đã chuẩn bị xong hàm huấn luyện và đánh giá cho mỗi epoch, ta có thể thiết lập quy trình huấn luyện chính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rLv98h_ZMcF"
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "mb = master_bar(range(EPOCHS))\n",
    "mb.names = ['Training loss', 'Validation loss', 'F1']\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "f1_scores = []\n",
    "x = []\n",
    "\n",
    "train_step = math.ceil(len(train_x)*1.0/BATCH_SIZE)\n",
    "val_step = math.ceil(len(validation_x)*1.0/BATCH_SIZE)\n",
    "for epoch in mb:\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    x.append(epoch)\n",
    "    \n",
    "    ### TODO 6.1\n",
    "    # Huấn luyện bằng hàm epoch_training()\n",
    "    # Sử dụng tập train_dataset\n",
    "    ### START CODE HERE\n",
    "    training_loss = None\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    # Báo cáo kết quả train/huấn luyện\n",
    "    mb.write('Finish train epoch {} with loss {:.4f}'.format(epoch, training_loss))\n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    ### TODO 6.2\n",
    "    # Đánh giá kết quả trên tập validation_dataset\n",
    "    # bằng hàm epoch_evaluation()\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    valid_loss, valid_score = None,None\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    # Báo cáo kết quả validate\n",
    "    mb.write('Finish validate epoch {} with loss {:.4f}, F1-score {:.4f}'.format(epoch,valid_loss, valid_score))\n",
    "    validation_losses.append(valid_loss)\n",
    "    f1_scores.append(valid_score)\n",
    "    \n",
    "    # Cập nhật đồ thị\n",
    "    global_step.assign_add(1)\n",
    "    mb.update_graph([[x, training_losses], [x, validation_losses], [x, f1_scores]], [0,EPOCHS], [0,1])\n",
    "    \n",
    "    # Update score và lưu model có score tốt nhất\n",
    "    if best_score < valid_score:\n",
    "        mb.write(\">>> Improved F1-score from {:.4f} to {:.4f}\".format(best_score, valid_score))\n",
    "        # Update best_score\n",
    "        best_score = valid_score\n",
    "        # Save model\n",
    "        model.save_weights(checkpoint_prefix.format(score=valid_score))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fApzh14xhVWg"
   },
   "source": [
    "### Đánh giá mô hình trên tập test có nhãn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FH4gGLJohy4k"
   },
   "source": [
    "Sau khi hoàn thành quá trình huấn luyện, ta sẽ đánh giá mô hình trên tập test đã chia ra từ dữ liệu có nhãn. Ta sử dụng thang điểm F1 để đánh giá độ chính xác của mô hình.\n",
    "\n",
    "> Tham khảo về định nghĩa F1-score ở đây: https://machinelearningcoban.com/2017/08/31/evaluation/#-f-score\n",
    "![alt text](https://machinelearningcoban.com/assets/33_evaluation/PR.png)\n",
    "\n",
    "Nếu được huấn luyện đúng cách thì kết quả F1 ở phần này sẽ rơi vào khoảng 70-80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oo5TtZdhmXOV"
   },
   "outputs": [],
   "source": [
    "test_loss, test_score = epoch_evaluation(model, test_dataset, mb, math.ceil(len(test_x)*1.0/BATCH_SIZE))\n",
    "print(\"F1-score on test set:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8iQ-HqwiHgZ"
   },
   "source": [
    "## Predict trên 3000 mẫu bình luận không nhãn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXsQxMmBgRHq"
   },
   "source": [
    "Đã xong quá trình huấn luyện và kiểm định mô hình trên dữ liệu có nhãn.\n",
    "\n",
    "Nếu thực hiện tốt các bài tập ở trên, thì đến đây bạn đã có một mô hình có thể phân tích cảm xúc của một đoạn văn bản tiếng Việt bất kì!\n",
    "\n",
    "(Tuy nhiên, nếu văn bản đó không phải là bình luận về quán ăn thì mô hình sẽ khó lòng dự đoán chính xác, bởi dữ liệu mà ta dùng để huấn luyện chỉ nằm trong miền các bình luận đánh giá quán ăn.)\n",
    "\n",
    "Việc cuối cùng là áp dụng mô hình để suy đoán cảm xúc của các mẫu bình luận không nhãn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEOVc6Fkhqw0"
   },
   "outputs": [],
   "source": [
    "#Các bình luận không nhãn\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj3fVO4cixsR"
   },
   "source": [
    "### TODO 7: Xây dựng hàm `predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQZ1Wxb-gAph"
   },
   "source": [
    "Ta cần viết một hàm nhận vào một câu/đoạn bình luận tiếng Việt, và trả về một trong 2 giá trị cảm xúc positive hoặc negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MWJTnlOg-8y"
   },
   "source": [
    "Cần lưu ý: các dữ liệu không nhãn chưa được tách thành từ nên các bạn cần sử dụng các thư viện hỗ trợ tokenize tiếng Việt (ví dụ underthesea) để tiền xử lý trong hàm `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGAD6lANiSQA"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from underthesea import word_tokenize\n",
    "except:\n",
    "    !pip install underthesea\n",
    "    from underthesea import word_tokenize\n",
    "    \n",
    "print('Bạn có thể ignore warning trên')\n",
    "   \n",
    "def predict(sentence, model, _word_list=words_list, _max_seq_length=MAX_SEQ_LENGTH):\n",
    "    \"\"\"\n",
    "    Dự đoán cảm xúc của một câu\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence: str\n",
    "        câu cần dự đoán\n",
    "    model: model keras\n",
    "        model keras đã được train/ load trọng số vừa train\n",
    "    _word_list: numpy.array\n",
    "        danh sách các từ đã biết\n",
    "    _max_seq_length: int\n",
    "        giới hạn số từ tối đa trong mỗi câu\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 nếu là negative, 1 nếu là positive\n",
    "    \"\"\"\n",
    "    ### TODO 7.1\n",
    "    # Tokenize/Tách từ trong câu\n",
    "    # Sử dụng hàm word_tokenize vừa import ở trên\n",
    "    ### START CODE HERE\n",
    "    tokenized_sent = None\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    ### TODO 7.2\n",
    "    # Đưa câu đã tokenize về dạng input_data thích hợp để truyền vào model\n",
    "    ### START CODE HERE\n",
    "    input_data = None\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    \n",
    "    ### TODO 7.3\n",
    "    # Truyền input_data qua model để nhận về xác suất các nhãn\n",
    "    # Chọn nhãn có xác suất cao nhất và return\n",
    "    ### START CODE HERE\n",
    "    predictions = None\n",
    "    return predictions\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEVsGBUPltXt"
   },
   "source": [
    "Load trọng số model đã train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHVSEPHNlsUf"
   },
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Uj9X5Pvmip6"
   },
   "source": [
    "Kiểm tra hàm vừa cài đặt, nếu cài đặt hàm và huấn luyện đúng thì kết quả lần lượt **_nên_** là 0 và 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrq7QAY9maNR"
   },
   "outputs": [],
   "source": [
    "print(\"Test 1:\",predict(\"Quán này rất dở\", model))\n",
    "print(\"Test 2:\",predict(\"Quán này rất ngon\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDlvsc6mm5V7"
   },
   "source": [
    "### Ghi kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P24nSZKWnxXD"
   },
   "source": [
    "Đọc file `sample_submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utKwdYdxmf9o"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(data_dir+\"/sample_submission.csv\", index_col=0)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqeetJ9tn5HZ"
   },
   "source": [
    "Duyệt từng dòng trong DataFrame `test_df` và dự đoán sau đó cập nhật kết quả vào `submission`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJ0wUC8sm_aa"
   },
   "outputs": [],
   "source": [
    "for _, row in tqdm(test_df.iterrows()):\n",
    "    submission.loc[row.id] = predict(row.text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Lq5HKGliOlZ"
   },
   "source": [
    "### Nộp bài"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13yv91bKiF1Y"
   },
   "source": [
    "Xuất ra file `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubrC3SpYohzX"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Dng060Lpp0W"
   },
   "source": [
    "Các bạn nộp file submission lên [Kaggle competition](https://www.kaggle.com/c/vietai5-as4/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VietAI Class 5: Assignment 4 - Sentiment Analysis",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
